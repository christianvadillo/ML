# -*- coding: utf-8 -*-
"""RNN_Mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l1TtNLK3AE9-tKYdPYtmiZrkySJ7iqIB

RNN models can be applied to images too. In general we can apply them to any data where there's a connnection between nearby units. Let's see how we can easily build a model that works with images.

Load the MNIST data, by now you should be able to do it blindfolded :)
reshape it so that an image looks like a long sequence of pixels
create a recurrent model and train it on the training data
how does it perform compared to a fully connected? How does it compare to Convolutional Neural Networks?
(feel free to run this exercise on a cloud GPU if it's too slow on your laptop
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import tensorflow.keras.backend as K

from tensorflow.keras.layers import LSTM, Dense, Input, TimeDistributed
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

(X_train, y_train), (X_test, y_test) = mnist.load_data()

plt.imshow(X_train[0])

# Reshapes data to 4D for Hierarchical RNN.
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

# Converts class vectors to binary class matrices.
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

X_train.shape

# Training parameters.
batch_size = 32
num_classes = 10
epochs = 10

# Embedding dimensions.
row_hidden = 128
col_hidden = 128

row, col, pixel = X_train.shape[1:]

K.clear_session()
# 4D input.
x = Input(shape=(row, col, pixel))

# Encodes a row of pixels using TimeDistributed Wrapper.
encoded_rows = TimeDistributed(LSTM(row_hidden))(x)

# Encodes columns of encoded rows.
encoded_columns = LSTM(col_hidden)(encoded_rows)

# Final predictions and model.
prediction = Dense(num_classes, activation='softmax')(encoded_columns)
model = Model(x, prediction)
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

# Training.
model.fit(X_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(X_test, y_test))

# Evaluation.
scores = model.evaluate(X_test, y_test, verbose=0)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])

